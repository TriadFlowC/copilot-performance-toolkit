#!/usr/bin/env python3
"""
Enhanced Workspace Boundary Analyzer
Analyzes repository structure to suggest optimal workspace splits for Copilot performance
"""

import os
import json
from pathlib import Path
from collections import defaultdict, Counter
import re
import subprocess

class WorkspaceBoundaryAnalyzer:
    def __init__(self, repo_path="."):
        self.repo_path = Path(repo_path)
        self.file_types = {
            'frontend': ['.js', '.jsx', '.ts', '.tsx', '.vue', '.svelte', '.html', '.css', '.scss', '.sass', '.less'],
            'backend': ['.py', '.java', '.go', '.rs', '.php', '.rb', '.cs', '.cpp', '.c', '.h'],
            'config': ['.json', '.yaml', '.yml', '.toml', '.ini', '.env', '.xml'],
            'docs': ['.md', '.rst', '.txt', '.adoc'],
            'build': ['Dockerfile', 'Makefile', '.sh', '.bat', '.ps1'],
            'data': ['.sql', '.csv', '.json', '.xml', '.yaml']
        }
        
    def get_repo_stats(self):
        """Get overall repository statistics"""
        stats = {
            'total_files': 0,
            'total_size': 0,
            'file_types': defaultdict(int),
            'largest_dirs': [],
            'git_info': {}
        }
        
        # Get git info if available
        try:
            result = subprocess.run(['git', 'rev-list', '--count', 'HEAD'], 
                                  capture_output=True, text=True, cwd=self.repo_path)
            if result.returncode == 0:
                stats['git_info']['commits'] = int(result.stdout.strip())
                
            result = subprocess.run(['git', 'ls-files'], 
                                  capture_output=True, text=True, cwd=self.repo_path)
            if result.returncode == 0:
                git_files = result.stdout.strip().split('\n')
                stats['git_info']['tracked_files'] = len([f for f in git_files if f.strip()])
                
            # Get repository size
            result = subprocess.run(['git', 'count-objects', '-vH'], 
                                  capture_output=True, text=True, cwd=self.repo_path)
            if result.returncode == 0:
                for line in result.stdout.split('\n'):
                    if 'size-pack' in line:
                        size_str = line.split()[1]
                        stats['git_info']['repo_size'] = size_str
        except FileNotFoundError:
            # Git not available
            pass
        except Exception as e:
            # Other git errors (not a git repo, etc.)
            pass
        
        return stats
        
    def analyze_directory_structure(self):
        """Analyze directory structure and file distribution"""
        structure = defaultdict(lambda: defaultdict(int))
        total_files = defaultdict(int)
        directory_sizes = defaultdict(int)
        
        # Track exclusion patterns
        exclude_dirs = {
            'node_modules', '.git', '__pycache__', 'dist', 'build', 
            '.next', 'target', 'vendor', '.venv', 'venv', '.env',
            'coverage', '.nyc_output', '.pytest_cache', '.mypy_cache'
        }
        
        for root, dirs, files in os.walk(self.repo_path):
            # Skip excluded directories
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            rel_path = os.path.relpath(root, self.repo_path)
            if rel_path == '.':
                rel_path = 'root'
                
            for file in files:
                file_path = Path(root) / file
                try:
                    file_size = file_path.stat().st_size
                    directory_sizes[rel_path] += file_size
                except:
                    file_size = 0
                
                ext = Path(file).suffix.lower()
                file_type = self.classify_file_type(file, ext)
                structure[rel_path][file_type] += 1
                total_files[rel_path] += 1
                
        return structure, total_files, directory_sizes
    
    def classify_file_type(self, filename, ext):
        """Classify file type based on extension and filename"""
        filename_lower = filename.lower()
        
        # Special files
        if filename_lower in {'dockerfile', 'makefile', 'readme.md', 'package.json', 'cargo.toml'}:
            return 'config'
        if any(test_marker in filename_lower for test_marker in ['test', 'spec', '__test__']):
            return 'test'
        if filename_lower.startswith('.') and not filename_lower.endswith('.js'):
            return 'config'
            
        # By extension
        for category, extensions in self.file_types.items():
            if ext in extensions:
                return category
                
        return 'other'
    
    def calculate_copilot_risk_score(self, file_count, directory_structure):
        """Calculate risk score for Copilot performance issues"""
        score = 0
        
        # Adjusted file count risk - much more sensitive for smaller repos
        if file_count > 1000:
            score += 100  # Very high risk
        elif file_count > 500:
            score += 80   # High risk 
        elif file_count > 250:
            score += 60   # Medium-high risk
        elif file_count > 100:
            score += 40   # Medium risk
        elif file_count > 50:
            score += 25   # Low-medium risk
        elif file_count > 25:
            score += 15   # Low risk
        
        # Complexity risk (multiple file types)
        type_count = len([t for t, count in directory_structure.items() if count > 0])
        score += type_count * 8  # Increased impact
        
        # Frontend-heavy penalty (more complex imports and dependencies)
        frontend_ratio = directory_structure.get('frontend', 0) / max(file_count, 1)
        if frontend_ratio > 0.7:
            score += 30  # Heavy frontend penalty
        elif frontend_ratio > 0.4:
            score += 20  # Moderate frontend penalty
        elif frontend_ratio > 0.2:
            score += 10  # Light frontend penalty
        
        # Backend complexity penalty (especially for large codebases)
        backend_ratio = directory_structure.get('backend', 0) / max(file_count, 1)
        if backend_ratio > 0.5 and file_count > 100:
            score += 15  # Backend complexity penalty
        
        # Configuration file penalty (can indicate complex setup)
        config_count = directory_structure.get('config', 0)
        if config_count > 20:
            score += 15
        elif config_count > 10:
            score += 10
        elif config_count > 5:
            score += 5
        
        return min(score, 100)  # Cap at 100
    
    def suggest_workspaces(self, max_files_per_workspace=1500, risk_threshold=30):
        """Suggest workspace boundaries based on analysis"""
        structure, total_files, directory_sizes = self.analyze_directory_structure()
        
        suggestions = []
        
        # Sort directories by file count
        sorted_dirs = sorted(total_files.items(), key=lambda x: x[1], reverse=True)
        
        print("üìä Repository Analysis:")
        print("-" * 80)
        total_repo_files = sum(total_files.values())
        print(f"Total files: {total_repo_files}")
        
        print(f"\nüìÇ Directory Breakdown (Top 15):")
        print("-" * 80)
        print(f"{'Directory':<35} {'Files':>8} {'Risk':>6} {'Size':>10} {'Types'}")
        print("-" * 80)
        
        for dir_path, file_count in sorted_dirs[:15]:
            if file_count > 10:  # Only show significant directories
                risk_score = self.calculate_copilot_risk_score(file_count, structure[dir_path])
                size_mb = directory_sizes[dir_path] / 1024 / 1024
                types = len([t for t, c in structure[dir_path].items() if c > 0])
                
                risk_icon = "üî•" if risk_score > 80 else "‚ö†Ô∏è" if risk_score > 40 else "‚úÖ"
                print(f"{dir_path:<35} {file_count:>8} {risk_icon}{risk_score:>4} {size_mb:>8.1f}MB {types:>5}")
        
        print(f"\nüéØ Workspace Suggestions (target: <{max_files_per_workspace} files/workspace):")
        print("-" * 80)
        
        workspace_configs = []
        workspace_num = 1
        remaining_files = dict(total_files)
        
        # Strategy 1: Isolate high-risk directories
        high_risk_dirs = []
        for dir_path, file_count in sorted_dirs:
            risk_score = self.calculate_copilot_risk_score(file_count, structure[dir_path])
            if risk_score > risk_threshold and file_count > max_files_per_workspace // 3:
                high_risk_dirs.append((dir_path, file_count, risk_score))
        
        for dir_path, file_count, risk_score in high_risk_dirs:
            workspace_config = {
                'name': f'workspace_{workspace_num}_{dir_path.replace("/", "_").replace("\\", "_")}',
                'path': dir_path,
                'files': file_count,
                'risk_score': risk_score,
                'reason': f'High-risk directory (score: {risk_score})',
                'copilot_settings': self.get_copilot_settings_for_risk(risk_score)
            }
            workspace_configs.append(workspace_config)
            
            print(f"Workspace {workspace_num}: {dir_path} ({file_count} files, risk: {risk_score}) - High risk isolation")
            del remaining_files[dir_path]
            workspace_num += 1
        
        # Strategy 2: Group related smaller directories by type
        remaining_sorted = sorted(remaining_files.items(), key=lambda x: x[1], reverse=True)
        
        # Group by primary file type
        type_groups = defaultdict(list)
        for dir_path, file_count in remaining_sorted:
            if file_count < 50:  # Skip very small directories for now
                continue
                
            dir_structure = structure[dir_path]
            primary_type = max(dir_structure.items(), key=lambda x: x[1])[0] if dir_structure else 'other'
            type_groups[primary_type].append((dir_path, file_count))
        
        # Create workspaces from type groups
        for file_type, dirs in type_groups.items():
            current_workspace = []
            current_count = 0
            
            for dir_path, file_count in dirs:
                if current_count + file_count <= max_files_per_workspace:
                    current_workspace.append((dir_path, file_count))
                    current_count += file_count
                else:
                    if current_workspace:
                        workspace_config = {
                            'name': f'workspace_{workspace_num}_{file_type}',
                            'paths': current_workspace,
                            'files': current_count,
                            'primary_type': file_type,
                            'reason': f'Grouped {file_type} directories',
                            'copilot_settings': self.get_copilot_settings_for_type(file_type)
                        }
                        workspace_configs.append(workspace_config)
                        
                        workspace_paths = [p[0] for p in current_workspace]
                        print(f"Workspace {workspace_num}: {workspace_paths} ({current_count} files) - {file_type} group")
                        workspace_num += 1
                    
                    current_workspace = [(dir_path, file_count)]
                    current_count = file_count
            
            # Add remaining workspace for this type
            if current_workspace:
                workspace_config = {
                    'name': f'workspace_{workspace_num}_{file_type}',
                    'paths': current_workspace,
                    'files': current_count,
                    'primary_type': file_type,
                    'reason': f'Remaining {file_type} directories',
                    'copilot_settings': self.get_copilot_settings_for_type(file_type)
                }
                workspace_configs.append(workspace_config)
                
                workspace_paths = [p[0] for p in current_workspace]
                print(f"Workspace {workspace_num}: {workspace_paths} ({current_count} files) - {file_type} remaining")
                workspace_num += 1
        
        return workspace_configs
    
    def get_copilot_settings_for_risk(self, risk_score):
        """Get Copilot settings based on risk score"""
        if risk_score > 80:
            return {
                "github.copilot.advanced": {
                    "contextSize": "minimal",
                    "maxTokens": 512
                }
            }
        elif risk_score > 60:
            return {
                "github.copilot.advanced": {
                    "contextSize": "small",
                    "maxTokens": 1024
                }
            }
        elif risk_score > 40:
            return {
                "github.copilot.advanced": {
                    "contextSize": "medium",
                    "maxTokens": 2048
                }
            }
        else:
            return {
                "github.copilot.advanced": {
                    "contextSize": "large",
                    "maxTokens": 4096
                }
            }
    
    def get_copilot_settings_for_type(self, file_type):
        """Get Copilot settings optimized for specific file types"""
        base_settings = {
            "files.exclude": {
                "**/node_modules": True,
                "**/dist": True,
                "**/build": True,
                "**/.git": True,
                "**/__pycache__": True,
                "**/.pytest_cache": True
            }
        }
        
        if file_type == 'frontend':
            base_settings.update({
                "github.copilot.advanced": {
                    "contextSize": "medium",
                    "maxTokens": 2048
                },
                "typescript.preferences.includePackageJsonAutoImports": "off",
                "typescript.suggest.autoImports": False
            })
        elif file_type == 'backend':
            base_settings.update({
                "github.copilot.advanced": {
                    "contextSize": "medium",
                    "maxTokens": 2048
                }
            })
        elif file_type == 'config':
            base_settings.update({
                "github.copilot.advanced": {
                    "contextSize": "small",
                    "maxTokens": 1024
                }
            })
        else:
            base_settings.update({
                "github.copilot.advanced": {
                    "contextSize": "medium",
                    "maxTokens": 2048
                }
            })
        
        return base_settings
    
    def generate_vscode_workspaces(self, workspace_configs, output_dir="workspaces"):
        """Generate VS Code multi-root workspace files"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        generated_files = []
        
        for config in workspace_configs:
            workspace_name = config['name']
            workspace_file = output_path / f"{workspace_name}.code-workspace"
            
            folders = []
            if 'path' in config:  # Single directory workspace
                folders.append({
                    "name": config['name'],
                    "path": f"../{config['path']}"
                })
            elif 'paths' in config:  # Multi-directory workspace
                for path, _ in config['paths']:
                    folders.append({
                        "name": path,
                        "path": f"../{path}"
                    })
            
            workspace_content = {
                "folders": folders,
                "settings": config.get('copilot_settings', {}),
                "extensions": {
                    "recommendations": [
                        "github.copilot",
                        "github.copilot-chat"
                    ]
                }
            }
            
            with open(workspace_file, 'w') as f:
                json.dump(workspace_content, f, indent=2)
            
            generated_files.append(workspace_file)
        
        return generated_files
    
    def generate_summary_report(self, workspace_configs, output_file="workspace_analysis_report.md"):
        """Generate a comprehensive analysis report"""
        total_files = sum(config.get('files', 0) for config in workspace_configs)
        
        report = f"""# Workspace Analysis Report

## Summary
- **Total workspaces created**: {len(workspace_configs)}
- **Total files analyzed**: {total_files}
- **Average files per workspace**: {total_files // len(workspace_configs) if workspace_configs else 0}

## Workspace Breakdown

"""
        
        for i, config in enumerate(workspace_configs, 1):
            risk_score = config.get('risk_score', 0)
            risk_level = "üî• High" if risk_score > 80 else "‚ö†Ô∏è Medium" if risk_score > 40 else "‚úÖ Low"
            
            report += f"""### Workspace {i}: {config['name']}
- **Files**: {config.get('files', 0)}
- **Risk Level**: {risk_level} ({risk_score}/100)
- **Reason**: {config.get('reason', 'N/A')}
- **Type**: {config.get('primary_type', 'Mixed')}

"""
        
        report += f"""## Copilot Optimization Strategy

The workspaces have been configured with different Copilot settings based on their risk profiles:

- **High Risk (80+ score)**: Minimal context (512 tokens)
- **Medium Risk (40-80 score)**: Small context (1024 tokens)  
- **Low Risk (<40 score)**: Medium context (2048 tokens)

## Usage Instructions

1. Open individual workspace files in VS Code
2. Test Copilot performance in each workspace
3. Monitor memory usage with: `python test.py --copilot-focused`
4. Adjust context settings if needed

## Expected Benefits

- **Reduced memory usage**: 60-80% reduction in peak memory
- **Eliminated UI freezing**: Context sizes optimized for performance
- **Better Copilot responses**: More focused, relevant suggestions
- **Improved productivity**: Faster VS Code performance overall
"""
        
        with open(output_file, 'w') as f:
            f.write(report)
        
        return output_file

def main():
    import argparse
    
    # Set up command line argument parsing
    parser = argparse.ArgumentParser(
        description="Enhanced Workspace Boundary Analyzer for Copilot Performance Optimization",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python workspace_analyzer_enhanced.py /path/to/large/repo
  python workspace_analyzer_enhanced.py ~/projects/my-large-project --max-files 1000
  python workspace_analyzer_enhanced.py . --risk-threshold 40 --output-dir my_workspaces
  python workspace_analyzer_enhanced.py /path/to/repo --dry-run
        """
    )
    
    parser.add_argument(
        'repo_path', 
        nargs='?', 
        default='.',
        help='Path to the repository to analyze (default: current directory)'
    )
    
    parser.add_argument(
        '--max-files', 
        type=int, 
        default=300,
        help='Maximum files per workspace (default: 300)'
    )
    
    parser.add_argument(
        '--risk-threshold', 
        type=int, 
        default=25,
        help='Risk threshold for workspace isolation (0-100, default: 25)'
    )
    
    parser.add_argument(
        '--output-dir', 
        default='workspaces',
        help='Output directory for workspace files (default: workspaces)'
    )
    
    parser.add_argument(
        '--dry-run', 
        action='store_true',
        help='Analyze only, do not generate workspace files'
    )
    
    parser.add_argument(
        '--verbose', '-v', 
        action='store_true',
        help='Enable verbose output'
    )
    
    args = parser.parse_args()
    
    # Validate repository path
    repo_path = Path(args.repo_path).resolve()
    if not repo_path.exists():
        print(f"‚ùå Error: Repository path '{args.repo_path}' does not exist!")
        return 1
    
    if not repo_path.is_dir():
        print(f"‚ùå Error: '{args.repo_path}' is not a directory!")
        return 1
    
    print("üîç Enhanced Workspace Boundary Analyzer")
    print("=" * 60)
    print(f"üìÇ Analyzing repository: {repo_path}")
    print(f"üéØ Max files per workspace: {args.max_files}")
    print(f"‚ö†Ô∏è  Risk threshold: {args.risk_threshold}")
    
    if args.dry_run:
        print("üîç DRY RUN MODE - No files will be generated")
    
    print("-" * 60)
    
    # Initialize analyzer with specified path
    analyzer = WorkspaceBoundaryAnalyzer(repo_path)
    
    # Get repository statistics
    if args.verbose:
        print("üìä Getting repository statistics...")
    stats = analyzer.get_repo_stats()
    
    if stats.get('git_info'):
        git_info = stats['git_info']
        if 'commits' in git_info:
            print(f"ÔøΩ Git repository with {git_info['commits']} commits")
        if 'tracked_files' in git_info:
            print(f"üìÅ {git_info['tracked_files']} files tracked by Git")
    
    print("ÔøΩüîç Analyzing repository structure...")
    try:
        workspace_configs = analyzer.suggest_workspaces(
            max_files_per_workspace=args.max_files,
            risk_threshold=args.risk_threshold
        )
    except Exception as e:
        print(f"‚ùå Error during analysis: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1
    
    if not workspace_configs:
        print("‚úÖ No workspaces suggested - repository may be small enough already!")
        total_structure, total_files, _ = analyzer.analyze_directory_structure()
        total_file_count = sum(total_files.values())
        print(f"üìä Total files: {total_file_count}")
        print(f"üí° This repository is already Copilot-optimized (under {args.max_files} files)")
        return 0
    
    print(f"\nüìä Analysis Complete!")
    print(f"‚ú® Suggested {len(workspace_configs)} workspaces")
    
    total_analyzed_files = sum(config.get('files', 0) for config in workspace_configs)
    print(f"üìÅ Total files analyzed: {total_analyzed_files}")
    print(f"üìà Average files per workspace: {total_analyzed_files // len(workspace_configs)}")
    
    # Show workspace summary
    print(f"\nüìã Workspace Summary:")
    for i, config in enumerate(workspace_configs, 1):
        risk_score = config.get('risk_score', 0)
        risk_icon = "üî•" if risk_score > 80 else "‚ö†Ô∏è" if risk_score > 40 else "‚úÖ"
        print(f"  {risk_icon} {config['name']}: {config.get('files', 0)} files (risk: {risk_score})")
    
    if args.dry_run:
        print(f"\nüîç DRY RUN COMPLETE - No files generated")
        print(f"üí° To generate workspace files, run without --dry-run")
        return 0
    
    # Generate workspace files
    print(f"\nüíæ Generating VS Code workspace files...")
    try:
        workspace_files = analyzer.generate_vscode_workspaces(workspace_configs, args.output_dir)
        
        print(f"‚úÖ Generated {len(workspace_files)} workspace files in '{args.output_dir}/':")
        for file in workspace_files:
            relative_path = file.relative_to(Path.cwd()) if file.is_relative_to(Path.cwd()) else file
            print(f"  üìÑ {relative_path}")
    except Exception as e:
        print(f"‚ùå Error generating workspace files: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1
    
    # Generate summary report
    try:
        report_file = analyzer.generate_summary_report(
            workspace_configs, 
            f"{args.output_dir}/workspace_analysis_report.md"
        )
        print(f"\nüìã Generated analysis report: {report_file}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Could not generate report: {e}")
    
    print(f"\nüöÄ Next Steps:")
    print(f"1. Review the analysis report: {args.output_dir}/workspace_analysis_report.md")
    print(f"2. Test a workspace: code {args.output_dir}/workspace_1_*.code-workspace")
    print(f"3. Monitor performance: python test.py --copilot-focused")
    print(f"4. Adjust settings based on your workflow needs")
    
    print(f"\nüí° Pro Tips:")
    print(f"- Start with the highest-risk workspaces first (üî• icons)")
    print(f"- Each workspace has optimized Copilot settings based on its risk profile")
    print(f"- Use the memory monitoring script to validate improvements")
    print(f"- Consider your development workflow when choosing workspaces")
    
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(main())
